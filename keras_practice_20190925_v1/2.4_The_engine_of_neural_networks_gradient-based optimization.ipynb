{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. The engine of neural networks: gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in the previous section, each neural layer from our first network example **transforms its input data as follows**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = relu(dot(W, input) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights ( Trainable parameters)\n",
    "- In this expression, **W and b are tensors** that are **attributes** of the layer. \n",
    "- They’re called **the weights** or **trainable parameters** of the layer (the kernel and bias attributes, respectively). \n",
    "- These weights contain the information learned by the network from exposure to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Random initialization\n",
    "\n",
    "- Initially, these weight matrices are filled with **small random values** (a step called **random initialization**). \n",
    "- Of course, there’s no reason to expect that relu(dot(W, input) + b), when W and b are random, will yield any useful representations. - The resulting representations are meaningless—but they’re a starting point. \n",
    "- What comes next is to gradually adjust these weights, based on a feedback signal. \n",
    "- This gradual adjustment, also called **training**, is basically the learning that machine learning is all about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens within what’s called a **training loop**, which works as follows.\n",
    "\n",
    "Repeat these steps in a loop, as long as necessary:\n",
    "\n",
    "1. Draw a batch (or a minibatch)) of training samples x and corresponding targets y.\n",
    "1. Run the network on x (a step called the **forward pass**) to obtain predictions y_pred.\n",
    "1. **Compute the loss** of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "1. **Update all weights** of the network in a way that slightly reduces the loss on this batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You’ll eventually end up with a network that has a very low loss on its training data: a low mismatch between predictions y_pred and expected targets y. \n",
    "- The network has “learned” to map its inputs to correct targets. -\n",
    "- From afar, it may look like magic, but when you reduce it to elementary steps, it turns out to be simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weigths of a neural network\n",
    "\n",
    "- Step 1 sounds easy enough—just I/O code (Input/Output). \n",
    "- Steps 2 and 3 are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned in the previous section. \n",
    "- The difficult part is step 4: updating the network’s weights. \n",
    "- Given an individual weight coefficient in the network, **how can you compute whether the coefficient should be increased or decreased, and by how much?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example:\n",
    "- One naive solution would be to freeze all weights in the network except the one scalar coefficient being considered, and try different values for this coefficient. \n",
    "- Let’s say the initial value of the coefficient is 0.3. \n",
    "- After the forward pass on a batch of data, the loss of the network on the batch is 0.5. \n",
    "- If you change the coefficient’s value to 0.35 and rerun the forward pass, the loss increases to 0.6. \n",
    "- But if you lower the coefficient to 0.25, the loss falls to 0.4. \n",
    "- In this case, it seems that updating the coefficient by -0.05 would contribute to minimizing the loss. \n",
    "- This would have to be repeated for all coefficients in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better to use Gradient Descent Method:\n",
    "\n",
    "- But such an approach would be horribly inefficient, because you’d need to compute two forward passes (which are expensive) for every individual coefficient (of which there are many, **usually thousands and sometimes up to millions**). \n",
    "- A much better approach is to take advantage of the fact that **all operations used in the network are differentiable**\n",
    "- **Compute the gradient of the loss** with regard to the network’s coefficients (weights). \n",
    "- You can then move the coefficients in **the opposite direction from the gradient**, thus decreasing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1. What’s a derivative?\n",
    "\n",
    "- Consider a **continuous, smooth function** f(x) = y, mapping a real number x to a new real number y. \n",
    "- Because the function is continuous, a small change in x can only result in a small change in y—that’s the intuition behind continuity.\n",
    "- Let’s say you increase x by a small factor epsilon_x: this results in a small epsilon_y change to y:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x + epsilon_x) = y + epsilon_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, because the function is smooth (its curve doesn’t have any abrupt angles), when epsilon_x is small enough, around a certain point p, it’s possible to approximate f as a linear function of slope a\n",
    "- epsilon_y becomes a * epsilon_x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x + epsilon_x) = y + a * epsilon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/diff.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this linear approximation is valid only when x is close enough to p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The slope a is called the **derivative** of f in p. \n",
    "- If a is negative, it means a small change of x around p will result in a decrease of f(x) (as shown in figure 2.10); and if a is positive, a small change in x will result in an increase of f(x). \n",
    "- Further, the absolute value of a (the magnitude of the derivative) tells you how quickly this increase or decrease will happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For every **differentiable function f(x)** (differentiable means “can be derived”: for example, smooth, continuous functions can be derived), **there exists a derivative function f'(x)** that maps values of x to the slope of the local linear approximation of f in those points. \n",
    "- For instance, the derivative of cos(x) is -sin(x), the derivative of f(x) = a * x is f'(x) = a, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you’re trying to update x by a factor epsilon_x in order to minimize f(x), and you know **the derivative of f**, then your job is done: \n",
    "- the derivative completely describes **how f(x) evolves as you change x**. \n",
    "- If you want to reduce the value of f(x), you just need to move x a little in the opposite direction from the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2. Derivative of a tensor operation: the gradient\n",
    "\n",
    "- A **gradient** is **the derivative of a tensor operation**. \n",
    "- It’s **the generalization of the concept of derivatives** to **functions of multidimensional inputs**: that is, to functions that take tensors as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider an input vector x, a matrix W, a target y, and a loss function loss. \n",
    "- You can use W to compute a target candidate y_pred\n",
    "- Compute the loss, or mismatch, between the target candidate y_pred and the target y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = dot(W, x)\n",
    "loss_value = loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data inputs x and y are frozen, then this can be interpreted as **a function mapping values of W** to loss values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_value = f(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s say the current value of W is W0. \n",
    "- Then **the derivative of f in the point W0** is a **tensor gradient(f)(W0)** with the same shape as W, where each coefficient gradient(f) (W0)[i, j] indicates the direction and magnitude of the change in loss_value you observe when modifying W0[i, j]. \n",
    "- That tensor gradient(f)(W0) is the gradient of the function **f(W) = loss_value** in W0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You saw earlier that the derivative of a function f(x) of a single coefficient can be interpreted as the **slope** of the curve of f.\n",
    "- Likewise, gradient(f)(W0) can be interpreted as the tensor describing the **curvature** of f(W) around W0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this reason, in much the same way that, for a function f(x), you can reduce the value of f(x) by moving x a little in the opposite direction from the derivative, with a function f(W) of a tensor, you can **reduce f(W) by moving W in the opposite direction from the gradient**: \n",
    "- For example, **W1 = W0 - step * gradient(f)(W0)** (where step (or also called \"**learning rate**\")is a small scaling factor). \n",
    "- That means going against the curvature, which intuitively should put you lower on the curve. \n",
    "- Note that the scaling factor step is needed because gradient(f)(W0) only approximates the curvature when you’re close to W0, so you don’t want to get too far from W0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 Stochastic gradient descent\n",
    "\n",
    "- Given a differentiable function, it’s theoretically possible to find its minimum analytically: \n",
    "- it’s known that a function’s minimum is **a point where the derivative is 0**, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function. \n",
    "- This can be done by **solving the equation gradient(f)(W) = 0 for W**. \n",
    "- This is a polynomial equation of N variables, where N is the number of coefficients in the network. \n",
    "- Although it would be possible to solve such an equation for N = 2 or N = 3, doing so is **intractable for real neural networks**, where the number of parameters is never less than a few thousand and can often be **several tens of millions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch stochastic gradient descent (mini-batch SGD)\n",
    "- Instead, you can use the four-step algorithm outlined at the beginning of this section: \n",
    "- Modify the parameters little by little based on the current loss value on a **random batch of data**.\n",
    "- Because you’re dealing with a differentiable function, you can compute its gradient, which gives you an efficient way to implement step 4. \n",
    "- If you update the weights in the opposite direction from the gradient, the loss will be a little less every time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw a batch of training samples x and corresponding targets y.\n",
    "1. Run the network on x to obtain predictions y_pred.\n",
    "1. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "1. Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "1. Move the parameters a little in the opposite direction from the gradient—for example W = step * gradient—thus reducing the loss on the batch a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What I just described is called \"**mini-batch stochastic gradient descent (mini-batch SGD)**\". \n",
    "- The term \"**stochastic**\" refers to the fact that each batch of data is drawn at random (stochastic is a scientific synonym of random). \n",
    "- Figure 2.11 illustrates what happens in 1D, when the network has only one parameter and you have only one training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "- As you can see, intuitively it’s **important to pick a reasonable value for the step factor (learning rate)**. \n",
    "- If it’s **too small**, the descent down the curve will take many iterations, and it could get **stuck in a local minimum**. \n",
    "- If step is **too large**, your updates may end up taking you to **completely random** locations on the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True SGD, mini-batch SGD, and batch SGD\n",
    "\n",
    "- Note that a variant of the mini-batch SGD algorithm would be to draw **a single sample and target at each iteration**, rather than drawing a batch of data. This would be **true SGD** (as opposed to mini-batch SGD). \n",
    "- Alternatively, going to the opposite extreme, you could run every step on **all data** available, which is called **batch SGD**. Each update would then be more accurate, but far more expensive. \n",
    "- The efficient compromise between these two extremes is to use mini-batches of reasonable size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although figure 2.11 illustrates gradient descent in a 1D parameter space, in practice you’ll use gradient descent in highly dimensional spaces: \n",
    "- every weight coefficient in a neural network is a free dimension in the space, and there may be tens of thousands or even millions of them. \n",
    "- To help you build intuition about loss surfaces, you can also visualize gradient descent along a 2D loss surface, as shown in figure 2.12. \n",
    "- But you can’t possibly visualize what the actual process of training a neural network looks like—you can’t represent a 1,000,000-dimensional space in a way that makes sense to humans. \n",
    "- As such, it’s good to keep in mind that the intuitions you develop through these low-dimensional representations may not always be accurate in practice. This has historically been a source of issues in the world of deep-learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opimization methods or Optimizers\n",
    "\n",
    "- Additionally, there exist multiple variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients. \n",
    "- There is, for instance, **SGD with momentum**, as well as **Adagrad, RMSProp**, and several others. Such variants are known as **optimization methods** or **optimizers**. \n",
    "- In particular, the concept of momentum, which is used in many of these variants, deserves your attention. \n",
    "- Momentum addresses two issues with SGD: convergence speed and local minima. \n",
    "- Consider figure 2.13, which shows the curve of a loss as a function of a network parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, around a certain parameter value, there is a local minimum: around that point, moving left would result in the loss increasing, but so would moving right. \n",
    "- If the parameter under consideration were being optimized via SGD with a small learning rate, then the optimization process would get stuck at the local minimum instead of making its way to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "- You can avoid such issues by using momentum, which draws inspiration from physics. A useful mental image here is to think of the optimization process as a small ball rolling down the loss curve. \n",
    "- If it has enough momentum, the ball won’t get stuck in a ravine and will end up at the global minimum. \n",
    "- **Momentum** is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past acceleration). \n",
    "- In practice, this means **updating the parameter w based not only on the current gradient value but also on the previous parameter update**, such as in this naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0efef5a6a925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpast_velocity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_current_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvelocity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_velocity\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "past_velocity = 0.\n",
    "momentum = 0.1\n",
    "while loss > 0.01:\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4. Chaining derivatives: the Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous algorithm, we casually assumed that because a function is differentiable, we can explicitly compute its derivative. \n",
    "- In practice, a neural network function consists of many tensor operations chained together, each of which has a simple, known derivative. \n",
    "- For instance, this is a network f composed of three tensor operations, a, b, and c, with weight matrices W1, W2, and W3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(W1, W2, W3) = a(W1, b(W2, c(W3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculus tells us that such a chain of functions can be derived using the following identity, called the **chain rule: f(g(x)) = f'(g(x)) * g'(x)**. \n",
    "- Applying the chain rule to the computation of the gradient values of a neural network gives rise to an algorithm called **Backpropagation** (also sometimes called reverse-mode differentiation). \n",
    "- Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter had in the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nowadays, and for years to come, people will implement networks in modern frameworks that are capable of **symbolic differentiation**, such as TensorFlow. \n",
    "- This means that, given a chain of operations with a known derivative, they can compute a gradient function for the chain (by applying the chain rule) that maps network parameter values to gradient values. \n",
    "- When you have access to such a function, the backward pass is reduced to a call to this gradient function. \n",
    "- Thanks to symbolic differentiation, you’ll never have to implement the Backpropagation algorithm by hand. \n",
    "- For this reason, we won’t waste your time and your focus on deriving the exact formulation of the Backpropagation algorithm in these pages. \n",
    "- All you need is a good understanding of how gradient-based optimization works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. Looking back at our first example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input images are stored in **Numpy tensors**, which are here formatted as float32 tensors of shape (60000, 784) (training data) and (10000, 784) (test data), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Our Neural Network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This network consists of a chain of two Dense layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. \n",
    "- Weight tensors, which are attributes of the layers, are where the knowledge of the network persists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network-compilation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss function : categorical_crossentropy (the loss function that’s used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize.)\n",
    "- This reduction of the loss happens via mini-batch stochastic gradient descent. \n",
    "- The exact rules governing a specific use of gradient descent are defined by the rmsprop optimizer passed as the first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2568 - acc: 0.9248\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1031 - acc: 0.9693\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0677 - acc: 0.9801\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0492 - acc: 0.9853\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0366 - acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221a21a2588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens when you call fit: the network will start to iterate on the training data in mini-batches of 128 samples, 5 times over (each iteration over all the training data is called an epoch). \n",
    "- At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. \n",
    "- After these 5 epochs, the network will have performed 2,345 gradient updates (469 per epoch), and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (Inference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 19us/step\n",
      "test_acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "- Learning means finding a combination of model parameters that minimizes a loss function for a given set of training data samples and their corresponding targets.\n",
    "\n",
    "- Learning happens by drawing random batches of data samples and their targets, and computing the gradient of the network parameters with respect to the loss on the batch. The network parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient.\n",
    "\n",
    "- The entire learning process is made possible by the fact that neural networks are chains of differentiable tensor operations, and thus it’s possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value.\n",
    "\n",
    "- Two key concepts you’ll see frequently in future chapters are loss and optimizers. These are the two things you need to define before you begin feeding data into a network.\n",
    "\n",
    "- The loss is the quantity you’ll attempt to minimize during training, so it should represent a measure of success for the task you’re trying to solve.\n",
    "\n",
    "- The optimizer specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp optimizer, SGD with momentum, and so on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
