{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. The gears of neural networks: tensor operations\n",
    "\n",
    "- All transformations learned by deep neural networks can be reduced to a handful of **tensor operations** applied to tensors of numeric data. \n",
    "- For instance, it’s possible to add tensors, multiply tensors, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial example, we were building our network by **stacking Dense layers** on top of each other. \n",
    "\n",
    "A Keras layer instance looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This layer can be interpreted as a function, which takes as input a 2D tensor and returns another 2D tensor—a new representation for the input tensor.\n",
    "\n",
    "- Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes of the layer):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = relu(dot(W, input) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s unpack this. \n",
    "\n",
    "**We have three tensor operations here:** \n",
    "- a dot product (dot) between the input tensor and a tensor named W; \n",
    "- an addition (+) between the resulting 2D tensor and a vector b; \n",
    "- and, finally, a relu operation. relu(x) is max(x, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Element-wise operations\n",
    "\n",
    "- The relu operation and addition are element-wise operations: \n",
    "- That is, operations that are applied independently to each entry in the tensors being considered. \n",
    "- This means these operations are highly amenable to massively parallel implementations (vectorized implementations, a term that comes from the vector processor supercomputer architecture from the 1970–1990 period). \n",
    "\n",
    "- If you want to write a naive Python implementation of an element-wise operation, you use a for loop, as in this naive implementation of an element-wise relu operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2                     #  x is a 2D Numpy tensor.\n",
    "\n",
    "    x = x.copy()                                 #  Avoid overwriting the input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do the same for addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape                # x and y are 2D Numpy tensors.\n",
    "\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the same principle, you can do element-wise multiplication, subtraction, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In practice, when dealing with Numpy arrays, these operations are available as well-optimized **built-in Numpy functions**\n",
    "\n",
    "- So, in Numpy, you can do the following element-wise operation, and it will be blazing fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-02d187609d12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m                       \u001b[1;31m# Element-wise Addition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# Element-wise relu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = x + y                       # Element-wise Addition\n",
    "\n",
    "z = np.maximum(z, 0.)           # Element-wise relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Broadcasting\n",
    "\n",
    "- Our earlier naive implementation of naive_add only supports the addition of **2D tensors with identical shapes**. \n",
    "- But in the **Dense layer** introduced earlier, **we added a 2D tensor with a vector**. \n",
    "\n",
    "### What happens with addition when the shapes of the two tensors being added differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When possible, and if there’s no ambiguity, \n",
    "\n",
    "- the smaller tensor will be broadcasted to match the shape of the larger tensor. \n",
    "\n",
    "### Broadcasting consists of two steps:\n",
    "\n",
    "1. Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "1. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s look at a concrete example. \n",
    "\n",
    "- Consider X with shape (32, 10) and y with shape (10,). \n",
    "- First, we add an empty first axis to y, whose shape becomes (1, 10). \n",
    "- Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape (32, 10), where Y[i, :] == y for i in range(0, 32). \n",
    "- At this point, we can proceed to add X and Y, because they have the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of implementation, no new 2D tensor is created, because that would be terribly inefficient. \n",
    "- The repetition operation is entirely virtual: **it happens at the algorithmic level rather than at the memory level**. \n",
    "- But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. \n",
    "\n",
    "### Here’s what a naive implementation would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2                           #  x is a 2D Numpy tensor\n",
    "    assert len(y.shape) == 1                           #  y is a Numpy vector.\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "\n",
    "    x = x.copy()                                       #  Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With broadcasting, you can generally apply two-tensor element-wise operations if one tensor has shape (a, b, ... n, n + 1, ... m) and the other has shape (n, n + 1, ... m). \n",
    "\n",
    "- The broadcasting will then automatically happen for axes a through n - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following example applies the element-wise maximum operation to two tensors of different shapes via broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10))        # x is a random tensor with shape (64, 3, 32, 10)\n",
    "y = np.random.random((32, 10))              #  y is a random tensor with shape (32, 10).\n",
    "\n",
    "z = np.maximum(x, y)                        # The output z has shape (64, 3, 32, 10) like x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3. Tensor dot\n",
    "\n",
    "- The **dot operation**, also called a **tensor product** (not to be confused with an element-wise product) is the most common, most useful tensor operation. \n",
    "- Contrary to element-wise operations, it combines entries in the input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An element-wise product is done with the * operator in Numpy, Keras, Theano, and TensorFlow. \n",
    "- dot uses a different syntax in TensorFlow, but in both Numpy and Keras it’s done using the standard dot operator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "z = np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras import backend as K\n",
    "z = K.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical notation, you’d note the operation with a dot (.):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = x . y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, what does the dot operation do? \n",
    "Let’s start with the dot product of two vectors x and y. \n",
    "\n",
    "It’s computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "     \n",
    "    # x and y are Numpy vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dot product between two vectors is a **scalar**. \n",
    "- Only vectors with **the same number** of elements are compatible for a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take **the dot product between a matrix x and a vector y**, which returns **a vector where the coefficients are the dot products between y and the rows of x**. \n",
    "\n",
    "\n",
    "You implement it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also reuse the code we wrote previously, which highlights the relationship between a matrix-vector product and a vector product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as soon as one of the two tensors has an ndim greater than 1, \n",
    "\n",
    "dot is no longer symmetric (commutative), which is to say that **dot(x, y) isn’t the same as dot(y, x)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Of course, a dot product generalizes to tensors with an arbitrary number of axes. \n",
    "- The most common applications may be the dot product between two matrices. \n",
    "- You can take the dot product of two matrices x and y (dot(x, y)) if and only if x.shape[1] == y.shape[0]. \n",
    "- The result is a matrix with shape (x.shape[0], y.shape[1]), where the coefficients are the vector products between the rows of x and the columns of y. \n",
    "\n",
    "Here’s the naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand dot-product shape compatibility, it helps to visualize the input and output tensors by aligning them as shown in figure 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x, y, and z are pictured as rectangles (literal boxes of coefficients). \n",
    "- Because the rows x and the columns of y must have the same size, it follows that the width of x must match the height of y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, you can take the dot product between higher-dimensional tensors, following the same rules for shape compatibility as outlined earlier for the 2D case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4. Tensor reshaping\n",
    "\n",
    "-  It is essential to understand is **tensor reshaping**. \n",
    "- Although it wasn’t used in the Dense layers in our first neural network example, we used it when we preprocessed the digits data before feeding it into our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reshaping a tensor means rearranging its rows and columns to match a target shape**. \n",
    "- Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. \n",
    "\n",
    "Reshaping is best understood via simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((6, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A special case of reshaping that’s commonly encountered is **transposition**. \n",
    "- Transposing a matrix means **exchanging its rows and its columns**, so that x[i, :] becomes x[:, i]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((300, 20))\n",
    "\n",
    "x = np.transpose(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.5. Geometric interpretation of tensor operations\n",
    "\n",
    "- Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, **all tensor operations have a geometric interpretation**. \n",
    "- For instance, let’s consider addition. We’ll start with the following vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0.5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector as an arrow linking the origin to the point, as shown in figure 2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s consider a new point, B = [1, 0.25], which we’ll add to the previous one. \n",
    "- This is done geometrically by chaining together the vector arrows, with the resulting location being the vector representing the sum of the previous two vectors (see figure 2.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig2-8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, **elementary geometric operations such as affine transformations, rotations, scaling, and so on can be expressed as tensor operations.** \n",
    "- For instance, a rotation of a 2D vector by an angle theta can be achieved via a dot product with a 2 × 2 matrix R = [u, v], where u and v are both vectors of the plane: u = [cos(theta), sin(theta)] and v = [-sin(theta), cos(theta)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.6. A geometric interpretation of deep learning\n",
    "\n",
    "You just learned that \n",
    "\n",
    "- **Neural networks consist entirely of chains of tensor operations**.\n",
    "- All of these tensor operations are just **geometric transformations of the input data**. \n",
    "- It follows that you can interpret a neural network as **a very complex geometric transformation in a high-dimensional space**, implemented via a long series of simple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 3D, the following mental image may prove useful. \n",
    "\n",
    "- Imagine two sheets of colored paper: one red and one blue. \n",
    "- Put one on top of the other. \n",
    "- Now crumple them together into a small ball. \n",
    "- That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. \n",
    "- What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. \n",
    "- With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2.9. Uncrumpling a complicated manifold of data\n",
    "<img src = \"images/Fig2-9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncrumpling paper balls is what machine learning is about: **finding neat representations for complex, highly folded data manifolds**. \n",
    "- At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. \n",
    "- Each layer in a deep network applies a transformation that disentangles the data a little—and a deep stack of layers makes tractable an extremely complicated disentanglement process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
