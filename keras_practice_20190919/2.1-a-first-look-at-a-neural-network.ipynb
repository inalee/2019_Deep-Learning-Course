{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first look at a neural network\n",
    "\n",
    "This notebook contains the code samples found in Chapter 2, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "We will now take a look at a **first concrete example of a neural network**, which makes use of the Python library Keras to learn **to classify \n",
    "hand-written digits**. \n",
    "\n",
    "Unless you already have experience with Keras or similar libraries, you will not understand everything about this \n",
    "first example right away. You probably haven't even installed Keras yet. Don't worry, that is perfectly fine. \n",
    "\n",
    "In the next chapter, we will \n",
    "review each element in our example and explain them in detail. \n",
    "\n",
    "So don't worry if some steps seem arbitrary or look like magic to you! \n",
    "We've got to start somewhere.\n",
    "\n",
    "- The problem we are trying to solve here is **to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9)**. \n",
    "\n",
    "- **The dataset** we will use is the **MNIST dataset**, a classic dataset in the machine learning community, which has been \n",
    "around for almost as long as the field itself and has been very intensively studied. \n",
    "- It's a set of **60,000 training images**, plus **10,000 test (6만개에서도 validation 데이터를 나눌 것임)\n",
    "images**, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. \n",
    "- You can think of \"solving\" MNIST \n",
    "as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. \n",
    "- As you become a machine \n",
    "learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset \n",
    "\n",
    "70% - train : train a model\n",
    "\n",
    "10% - validation : model selection\n",
    "*** validation 데이터는 꼭 있어야함!!!!!!★★\n",
    "\n",
    "10% - test : generalization(성능, 퍼포먼스를 measure)\n",
    "\n",
    "test 데이터에서는 어떠한 insight 도 가져와서는 안됨. 나중에 모델을 고르고나서 테스트 하는 것이 테스트 데이터. 그 전에는 절대 건드리면 안됨!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "- weight 로 구성. weight를 업데이트 하는 것이 training. 즉, 최적의 weight를 찾는 것이 궁극적인 목표.\n",
    "- 예측값과 정답의 값 차이가 가장 적게끔 하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset, Test dataset, validation dataset\n",
    "- `train_images` and `train_labels` form the \"training set\", the data that the model will learn from. \n",
    "- The model will then be tested on the \n",
    "\"test set\", `test_images` and `test_labels`. \n",
    "- Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging \n",
    "from 0 to 9. \n",
    "- There is a one-to-one correspondence between the images and the labels.\n",
    "\n",
    "Let's have a look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape ### 6만개가 있고 크기는 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images[0]) ### 길이가 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our workflow will be as follow: \n",
    "- first we will present our neural network with the training data, `train_images` and `train_labels`. \n",
    "- The \n",
    "network will then learn to associate images and labels. \n",
    "- Finally, we will ask the network to produce predictions for `test_images`, and we \n",
    "will verify if these predictions match the labels from `test_labels`.\n",
    "\n",
    "##Let's build our network -- again, remember that you aren't supposed to understand everything about this example just yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKFLOW\n",
    "\n",
    "\n",
    "### 1.problem define\n",
    "- 너무 데이터에만 치중되어 있으면 재미가 없어질 수 있음. 따라서 어떤 문제를 풀 수 있는지에 대해서 생각해보기\n",
    "\n",
    "### 2. data availability\n",
    "\n",
    "\n",
    "### 3. data preparation \n",
    "- collection\n",
    "- preprocessing\n",
    "\n",
    "### 4. NN architecture design (input / output)\n",
    "- data representation\n",
    "\n",
    "### 5. training\n",
    "\n",
    "### 6. inference with test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,))) ## 이미지가 28*28 모양이므로 이거를 하나로 폈음\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fully connected layer : 이전 노드와 모두 연결되어 있는 것 (Dense)\n",
    "- dense 512 : 이전 노드가 512개(?)\n",
    "\n",
    "#### dense 10 :  0부터 9까지의 숫자중 어디에 속하는지에 대한 확률을 나오게 하고 싶기 때문에..... 각 뉴런이 각 클래스에 속할 확률을 계산하기 위함. 즉 input data가 0~9 일 확률을 각 노드별로 나타내야 하므로 10개.\n",
    "\n",
    "\n",
    "- 인풋노드 28*28개 가 인풋레이어\n",
    "- 512개의 노드를 가진 히든레이어\n",
    "- 10개의 노드를 가진 아웃풋레이어\n",
    "\n",
    "\n",
    "#### Relu :  MAX(X,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of Deep Neural Networks\n",
    "- The core building block of neural networks is the \"**layer**\", a data-processing module which you can conceive as a \"filter\" for data. \n",
    "- Some \n",
    "data comes in, and comes out in a more useful form. \n",
    "- Precisely, layers extract **representations** out of the data fed into them -- hopefully \n",
    "**representations** that are more meaningful for the problem at hand. \n",
    "- Most of deep learning really consists of chaining together simple layers \n",
    "which will implement a form of progressive \"data distillation\". \n",
    "- A deep learning model is like a sieve for data processing, made of a \n",
    "succession of increasingly refined data filters -- the \"layers\".\n",
    "\n",
    "**Example**: \n",
    "- Here our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers. \n",
    "(weight 가 있는 layer 만 얘기해서 input layer는 제외했음)\n",
    "\n",
    "- The second (and last) layer is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). \n",
    "\n",
    "\n",
    "-> multiclass classification 에서 activation function으로 사용된다.\n",
    "\n",
    "\n",
    "- Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
    "\n",
    "\n",
    "-> 10개의 클래스중에 해당 클래스에 input 이 속할 확률. 즉 확률 값을 10개를 갖게되며, 가장 높은 확률이 해당 input의 예측 값이 됨.\n",
    "\n",
    "\n",
    "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
    "\n",
    "* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be \n",
    "\n",
    "- 손실함수(error function) : prediction한 값과 실제 정답 값의 ※차이※를 정의해주는 함수. 주로 cross-entropy/유클리디안 loss 많이 사용함. \n",
    "- documentation 참고\n",
    "\n",
    "\n",
    "able to steer itself in the right direction.\n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "\n",
    "-> weight 을 학습하는 방법을 정의...\n",
    "\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n",
    "classified).\n",
    "\n",
    "-> metric: performance measurement\n",
    "\n",
    "The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer로 구성되어 있음. (input - hidden - output)\n",
    "- hidden layer 가 많으면 왜 성능이 좋을까?: 복잡한 형태를 표현할 수 있고 high-level feature를 학습할 수 있음.\n",
    "- 뉴럴 네트워크는 unkown knowledge 를 데이터 기반으로 feature를 스스로 학습할 수 있는 것이 큰 장점\n",
    "- data가 충분히 있어야 좋음! data가 충분히 있다는 가정이 있으면 성능이 좋음.\n",
    "- 블랙박스 모델...(문제는 있음) -> 뭐때문에 이렇게됐는지는 잘 모르지만....해석이 잘안됨.. \n",
    "- ★ 데이터 충분하고 아키텍쳐가 그것을 충분히 표현할 수 있다는 것이 가장 기본 가정 ★\n",
    "\n",
    "\n",
    "##### deep learning = representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 컴파일링\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']) #performance measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Preprocessiong\n",
    "- Convert Input data shape : Before training, we will preprocess our data by reshaping it into the shape that the network expects.\n",
    "\n",
    "-> 28*28 2D형태이므로 하나의 vector 형태로 만들어주는 전처리가 필요함\n",
    "\n",
    "\n",
    "- Normalization: Scaling it so that all values are in the `[0, 1]` interval. \n",
    "\n",
    "-> 픽셀하나당 0~255 값을 가짐(이미지).\n",
    "\n",
    "\n",
    "Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \n",
    "values in the `[0, 255]` interval. \n",
    "\n",
    "We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28)) \n",
    "train_images = train_images.astype('float32') / 255 ## 0~255 사이 이므로 255로 scaling 해줌.\n",
    "\n",
    "##train data를 스케일링 해주었으면 test data도 똑같이 스케일링 해주어야함.\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to **categorically encode the labels**, a step which we explain in chapter 3 (multi-class classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical ### one-hot vector\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2541 - accuracy: 0.9264\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1015 - accuracy: 0.9702\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0672 - accuracy: 0.9798\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0504 - accuracy: 0.9853\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0374 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cb8013d240>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch: 내 뉴럴네트워크 전체를 트레이닝 하는 수\n",
    "#### batch_size: 128-> 한번에 feed 되는 샘플의 수는 128개씩 넣을거야 라는 얘기\n",
    "\n",
    "- 128개 넣어서 그 loss 값을 weight을 한번 업데이트한다.\n",
    "\n",
    "- 60000/128 : 469 iteration을 해야 one epoch인 것임..\n",
    "\n",
    "-> 즉 128 배치 기준으로 469 iteration 돌려야 one epoch을 학습한것임~~~~~~\n",
    "\n",
    "-> epoch 이 다섯개이므로 ,,,,,,,,,,,,, 이 과정을 다섯번해야함..\n",
    "\n",
    "-> 미니배치 사이즈 설정을 잘해주어야함... 우리는 10개 class 보는 네트워크인데..배치사이즈를 3개로한다? -> 말이안됨.. 3개만보고 weight를 업데이트 하는거니까..그래서 적어도 10개는 되어야함.해당 상황에서는.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over \n",
    "the training data.\n",
    "\n",
    "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9821000099182129\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy. \n",
    "This gap between training accuracy and test accuracy is an example of \"**overfitting**\", \n",
    "the fact that machine learning models tend to perform worse on new data than on their training data. \n",
    "Overfitting will be a central topic in chapter 3.\n",
    "\n",
    "- This concludes our very first example -- you just saw how we could build and a train a neural network to classify handwritten digits, in \n",
    "less than 20 lines of Python code. \n",
    "- In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really \n",
    "going on behind the scenes. \n",
    "- You will learn about \"tensors\", the data-storing objects going into the network, about tensor operations, which \n",
    "layers are made of, and about gradient descent, which allows our network to learn from its training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오버피팅\n",
    "- 학습에 있어서 중요하지 않은 패턴까지 외워버리는 경우.\n",
    "- 학습되어야 할 데이터가 적게 주어진 경우 발생할 수 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
