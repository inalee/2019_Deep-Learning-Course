{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"images/ch3_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This chapter is designed to get you started with **using neural networks to solve real problems**. \n",
    "- You’ll consolidate the knowledge you gained from our first practical example in chapter 2\n",
    "- You’ll apply what you’ve learned to three new problems covering **the three most common use cases of neural networks**: \n",
    "   1. binary classification\n",
    "   1. multiclass classification\n",
    "   1. scalar regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will take a closer look at the core components of neural networks: **layers, networks, objective functions, and optimizers**. \n",
    "- We’ll give you a quick introduction to Keras, the Python deep-learning library that we’ll use throughout this course. \n",
    "- We’ll dive into three introductory examples of how to use neural networks to address real problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1). Classifying movie reviews as positive or negative (binary classification)\n",
    "    2). Classifying news wires by topic (multiclass classification)\n",
    "    3). Estimating the price of a house, given real-estate data (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this chapter, you’ll be able to use neural networks to solve simple machine problems such as classification and regression over vector data. You’ll then be ready to start building a more principled, theory-driven understanding of machine learning in chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Anatomy of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in the previous chapters, training a neural network revolves around the following objects:\n",
    "- **Layers**, which are combined into a network (or model)\n",
    "- **The input data** and corresponding **targets**\n",
    "- **The loss function**, which defines the feedback signal used for learning\n",
    "- **The optimizer**, which determines how learning proceeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize their interaction as illustrated in figure 3.1: the network, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the network’s predictions match what was expected. The optimizer uses this loss value to update the network’s weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig3-1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at layers, networks, loss functions, and optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1. Layers: the building blocks of deep learning\n",
    "\n",
    "- The fundamental data structure in neural networks is the layer. \n",
    "- **A layer** is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. \n",
    "- Some layers are stateless, but **more frequently layers have a state: the layer’s weights**, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge.\n",
    "\n",
    "@ Examples of Stateless layers (No weigths) : Flatten, Pooling, and Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different layers are appropriate for different tensor formats and different types of data processing. \n",
    "- For instance, **simple vector data**, stored in **2D tensors of shape (samples, features)**, is often processed by densely connected layers, also called **fully connected** or **dense layers** (the Dense class in Keras). \n",
    "- **Sequence data**, stored in **3D tensors of shape (samples, timesteps, features)**, is typically processed by **recurrent layers** such as an **LSTM layer**. \n",
    "- Image data, stored in **4D tensors**, is usually processed by **2D convolution layers (Conv2D)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by frameworks like Keras. \n",
    "- Building deep-learning models in Keras is done by **clipping together compatible layers to form useful data-transformation pipelines** (케라스에서는 호환 가능한 층들을 엮어 데이터 변환 pipline을 구성함). \n",
    "- The notion of **layer compatibility** here refers specifically to the fact that every layer will **only accept input tensors of a certain shape and will return output tensors of a certain shape**. \n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "layer = layers.Dense(32, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We’re creating a layer that will only accept as input 2D tensors where the first dimension is 784 (**axis 0, the batch dimension, is unspecified, and thus any value would be accepted**). \n",
    "- This layer will return a tensor where the first dimension has been transformed to be 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. \n",
    "- When using Keras, you don’t have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. \n",
    "\n",
    "For instance, suppose you write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "model.add(layers.Dense(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second layer didn’t receive an input shape argument**—instead, it automatically inferred its input shape as being the output shape of the layer that came before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2. Models: networks of layers\n",
    "\n",
    "- A deep-learning model is a **directed, acyclic graph (DAG)** of layers. \n",
    "- The most common instance is a linear stack of layers, mapping a single input to a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you move forward, you’ll be exposed to a much broader variety of network topologies. Some common ones include the following:\n",
    "\n",
    "- Two-branch networks\n",
    "- Multihead networks\n",
    "- Inception blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The topology of a network defines a **hypothesis space**. \n",
    "- Defined machine learning \"**Searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal**\". (가능성 있는 공간을 사전에 정의하고 피드백 신호의 도움을 받아 입력 데이터에 대한 유용한 변환을 찾는 것)\n",
    "- By choosing a network topology, you **constrain your space of possibilities (hypothesis space)** to a specific series of tensor operations, mapping input data to output data. \n",
    "- What you’ll then be searching for is **a good set of values for the weight tensors** involved in these tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Picking the right network architecture is more an art than a science (No theoretical proof) \n",
    "- Although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect (Know-how). \n",
    "\n",
    "The next few chapters will both teach you explicit principles for building neural networks and help you develop intuition as to what works or doesn’t work for specific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3. Loss functions and optimizers: keys to configuring the learning process\n",
    "\n",
    "Once the network architecture is defined, you still have to choose two more things:\n",
    "\n",
    "- **Loss function (objective function)**— The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "- **Optimizer**— Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiloss networks:\n",
    "- A neural network that has multiple outputs may have **multiple loss functions (one per output)**. \n",
    "- But the gradient-descent process must be based on **a single scalar loss value**; \n",
    "- So, for multiloss networks, all losses are combined (via averaging) into **a single scalar quantity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function (Loss function):\n",
    "\n",
    "- **Choosing the right objective function for the right problem is extremely important**: your network will take any shortcut it can, to minimize the loss; so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. \n",
    "- **Just remember that all neural networks you build will be just as ruthless in lowering their loss function**—so choose the objective wisely, or you’ll have to face unintended side effects.\n",
    "\n",
    "- Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. \n",
    "- For instance, you’ll use \n",
    "  1. **Binary crossentropy** for a two-class classification problem, \n",
    "  1. **Categorical crossentropy** for a many-class classification problem,\n",
    "  1. **Mean-squared error** for a regression problem, \n",
    "  1. **Connectionist temporal classification (CTC)** for a sequence-learning problem, and so on. \n",
    "\n",
    "\n",
    "- Only when you’re working on truly new research problems will you have to develop your own objective functions. \n",
    "- In the next few chapters, we’ll detail explicitly which loss functions to choose for a wide range of common tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Introduction to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras (https://keras.io) is a deep-learning framework for Python that provides a convenient way to define and train almost any kind of deep-learning model. \n",
    "\n",
    "Keras was initially developed for researchers, with the aim of enabling fast experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key features of Keras:\n",
    "\n",
    "- It allows the same code to run seamlessly on CPU or GPU.\n",
    "- It has a user-friendly API that makes it easy to quickly prototype deep-learning models.\n",
    "- It has built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n",
    "- It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. This means Keras is appropriate for building essentially any deep-learning model, from a generative adversarial network to a neural Turing machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is distributed under the permissive MIT license, which means it can be freely used in commercial projects. It’s compatible with any version of Python from 2.7 to 3.6 (as of mid-2017).\n",
    "\n",
    "Keras has well over 200,000 users, ranging from academic researchers and engineers at both startups and large companies to graduate students and hobbyists. \n",
    "\n",
    "Keras is used at Google, Netflix, Uber, CERN, Yelp, Square, and hundreds of startups working on a wide range of problems. \n",
    "\n",
    "Keras is also a popular framework on Kaggle, the machine-learning competition website, where almost every recent deep-learning competition has been won using Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig3-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 Deep Learning Frameworks:\n",
    "https://www.einfochips.com/blog/deep-learning-frameworks/\n",
    "\n",
    "<img src = \"images/Fig3-2_1.png\">\n",
    "<img src = \"images/Fig3-2_2.png\">\n",
    "<img src = \"images/Fig3-2_3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig3-2_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. Keras, TensorFlow, Theano, and CNTK\n",
    "\n",
    "Keras is a model-level library, providing high-level building blocks for developing deep-learning models. \n",
    "\n",
    "It doesn’t handle low-level operations such as tensor manipulation and differentiation. \n",
    "\n",
    "Instead, it relies on a **specialized, well-optimized tensor library** to do so, serving as the **backend engine** of Keras. \n",
    "\n",
    "Rather than choosing a single tensor library and tying the implementation of Keras to that library, Keras handles the problem in a modular way (see figure 3.3); \n",
    "\n",
    "Thus several different backend engines can be plugged seamlessly into Keras. \n",
    "\n",
    "Currently, the three existing backend implementations are the **TensorFlow backend**, the **Theano backend**, and the Microsoft Cognitive Toolkit **(CNTK) backend**. \n",
    "\n",
    "In the future, it’s likely that Keras will be extended to work with even more deep-learning execution engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Fig3-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow, CNTK, and Theano are some of the primary platforms for deep learning today. \n",
    "\n",
    "Theano (http://deeplearning.net/software/theano) is developed by the MILA lab at Université de Montréal, \n",
    "TensorFlow (www.tensorflow.org) is developed by Google, and \n",
    "CNTK (https://github.com/Microsoft/CNTK) is developed by Microsoft. \n",
    "\n",
    "**Any piece of code that you write with Keras can be run with any of these backends without having to change anything in the code**: \n",
    "- you can seamlessly switch between the two during development, which often proves useful—for instance, if one of these backends proves to be faster for a specific task. \n",
    "- We recommend using the TensorFlow backend as the default for most of your deep-learning needs, because it’s the most widely adopted, scalable, and production ready.\n",
    "\n",
    "Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both CPUs and GPUs. \n",
    "\n",
    "When running on CPU, TensorFlow is itself wrapping a low-level library for tensor operations called Eigen (http://eigen.tuxfamily.org). \n",
    "\n",
    "On GPU, Tensor-Flow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (**cuDNN**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2. Developing with Keras: a quick overview\n",
    "\n",
    "You’ve already seen one example of a Keras model: the MNIST example. \n",
    "\n",
    "The typical Keras workflow looks just like that example:\n",
    "\n",
    "1. Define your training data: input tensors and target tensors.\n",
    "1. Define a network of layers (or model) that maps your inputs to your targets.\n",
    "1. Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.\n",
    "1. Iterate on your training data by calling the fit() method of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to define a model:\n",
    "\n",
    "- There are two ways to define a model: \n",
    "  - Using the **Sequential class** (only for linear stacks of layers, which is the most common network architecture by far) \n",
    "  - The **functional API** (for directed acyclic graphs of layers, which lets you build completely arbitrary architectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1).  A two-layer model defined using the Sequential class:\n",
    "\n",
    "Note that we’re passing the expected shape of the input data to the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2).  A two-layer model defined using the functional API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the functional API, you’re manipulating the data tensors that the model processes and applying layers to this tensor as if they were functions. (함수형 API를 사용하면 모델이 처리할 데이터 텐서를 만들고 마치 함수처럼 이 텐서에 층을 적용함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/note_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be using the Sequential class in our code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model architecture is defined, it doesn’t matter whether you used a Sequential model or the functional API. \n",
    "\n",
    "All of the following steps are the same.\n",
    "\n",
    "The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. \n",
    "\n",
    "Here’s an example with a single loss function, which is by far the most common case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the **fit() method**, \n",
    "- It is similar to what you would do in Scikit-Learn and several other machine-learning libraries: (사이킷런의 API 중 학습을 하는 fit() 메서드와 예측을 만드는 predict() 메서드가 케라스에서 같은 이름과 역할로 사용됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll look at three basic examples in sections 3.4, 3.5, and 3.6: a two-class classification example, a many-class classification example, and a regression example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
